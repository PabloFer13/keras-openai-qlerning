{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning y Deep Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Pablo Melendez\n",
    "- ### Hector Maga√±a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el algoritmo de Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_accion(Q, s, eps=0.1):\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        return np.argmax(Q[s])\n",
    "\n",
    "def QL(env, n_iter=10000, gamma=0.95, alfa=0.01, eps=0.3, d_eps=0.00005):\n",
    "    \n",
    "    n_a = env.action_space.n\n",
    "    n_s = env.observation_space.n\n",
    "\n",
    "    Q = np.zeros((n_s, n_a))\n",
    "#     print(Q.shape)\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        done = False        \n",
    "        state = env.reset()\n",
    "        \n",
    "        if eps > 0.01:\n",
    "            eps -= d_eps\n",
    "        \n",
    "        while not done:\n",
    "            action = sel_accion(Q, state, eps)\n",
    "            next_state, rew, done, info = env.step(action)\n",
    "#             print('-----')\n",
    "#             print(action)\n",
    "#             print(rew)\n",
    "#             print(info)\n",
    "#             print('-----')\n",
    "\n",
    "            Q[state,action] = Q[state,action] + alfa*(rew + gamma*np.max(Q[next_state]) - Q[state,action])\n",
    "            state = next_state\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el algoritmo de Deep Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-00a6e5a2a694>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-00a6e5a2a694>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    model.add(Dense(16))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def DeepQL(env, mem_limit = 1000, batch = 50, warmup = 2000, steps = 50000):\n",
    "\n",
    "    # Semilla para valores aleatorios\n",
    "    np.random.seed(123)\n",
    "    env.seed(123)\n",
    "    \n",
    "    # Obtenemos la cantidad de acciones\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    # Primera capa\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_actions, input_shape=(env.observation_space.shape,))\n",
    "              \n",
    "    # Capas siguientes\n",
    "#     model.add(Dense(16))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "#     model.add(Activation('linear'))\n",
    "              \n",
    "    # Estructura de la red\n",
    "    print(model.summary())\n",
    "\n",
    "    # Creamos la poliza inicial y compilamos el agente\n",
    "    memory = SequentialMemory(limit = mem_limit, window_length = 1)\n",
    "              \n",
    "    policy = BoltzmannQPolicy()\n",
    "              \n",
    "    dqn = DQNAgent(model=model, nb_actions = nb_actions, memory = memory, nb_steps_warmup = warmup,\n",
    "                   target_model_update = 1e-2, policy = policy)\n",
    "              \n",
    "    dqn.compile(Adam(lr = 1e-3), metrics = ['mae'])\n",
    "\n",
    "    # Entrenamos al agente\n",
    "    dqn.fit(env, nb_steps = steps, visualize = False, verbose = 0)\n",
    "\n",
    "    # Respaldo de los pesos finales\n",
    "    # dqn.save_weights('dqn_{}_weights.h5f'.format(name), overwrite = True)\n",
    "\n",
    "    # Testeamos el resultado\n",
    "    dqn.test(env, nb_episodes = 50, visualize = False)\n",
    "    \n",
    "    # Regresamos el agente entrenado\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make('FrozenLake-v0')\n",
    "\n",
    "test_env.reset()\n",
    "\n",
    "print(test_env.observation_space)\n",
    "print(test_env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 4)                 8         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 428\n",
      "Trainable params: 428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 3\n",
      "Episode 2: reward: 0.000, steps: 5\n",
      "Episode 3: reward: 0.000, steps: 4\n",
      "Episode 4: reward: 0.000, steps: 9\n",
      "Episode 5: reward: 0.000, steps: 2\n",
      "Episode 6: reward: 0.000, steps: 6\n",
      "Episode 7: reward: 0.000, steps: 5\n",
      "Episode 8: reward: 0.000, steps: 4\n",
      "Episode 9: reward: 0.000, steps: 6\n",
      "Episode 10: reward: 0.000, steps: 6\n",
      "Episode 11: reward: 0.000, steps: 3\n",
      "Episode 12: reward: 0.000, steps: 3\n",
      "Episode 13: reward: 0.000, steps: 2\n",
      "Episode 14: reward: 1.000, steps: 8\n",
      "Episode 15: reward: 0.000, steps: 6\n",
      "Episode 16: reward: 0.000, steps: 2\n",
      "Episode 17: reward: 0.000, steps: 9\n",
      "Episode 18: reward: 0.000, steps: 8\n",
      "Episode 19: reward: 0.000, steps: 4\n",
      "Episode 20: reward: 1.000, steps: 12\n",
      "Episode 21: reward: 0.000, steps: 5\n",
      "Episode 22: reward: 0.000, steps: 4\n",
      "Episode 23: reward: 0.000, steps: 6\n",
      "Episode 24: reward: 1.000, steps: 10\n",
      "Episode 25: reward: 0.000, steps: 2\n",
      "Episode 26: reward: 0.000, steps: 4\n",
      "Episode 27: reward: 0.000, steps: 5\n",
      "Episode 28: reward: 0.000, steps: 2\n",
      "Episode 29: reward: 0.000, steps: 7\n",
      "Episode 30: reward: 0.000, steps: 9\n",
      "Episode 31: reward: 0.000, steps: 8\n",
      "Episode 32: reward: 0.000, steps: 5\n",
      "Episode 33: reward: 0.000, steps: 6\n",
      "Episode 34: reward: 0.000, steps: 5\n",
      "Episode 35: reward: 0.000, steps: 3\n",
      "Episode 36: reward: 0.000, steps: 21\n",
      "Episode 37: reward: 0.000, steps: 6\n",
      "Episode 38: reward: 0.000, steps: 5\n",
      "Episode 39: reward: 1.000, steps: 15\n",
      "Episode 40: reward: 1.000, steps: 10\n",
      "Episode 41: reward: 0.000, steps: 3\n",
      "Episode 42: reward: 0.000, steps: 6\n",
      "Episode 43: reward: 0.000, steps: 4\n",
      "Episode 44: reward: 0.000, steps: 3\n",
      "Episode 45: reward: 0.000, steps: 3\n",
      "Episode 46: reward: 0.000, steps: 3\n",
      "Episode 47: reward: 0.000, steps: 7\n",
      "Episode 48: reward: 0.000, steps: 9\n",
      "Episode 49: reward: 0.000, steps: 4\n",
      "Episode 50: reward: 0.000, steps: 6\n"
     ]
    }
   ],
   "source": [
    "test_env.reset()\n",
    "DQr = DeepQL(test_env, mem_limit = 1000, batch = 10, warmup = 10, steps = 10)\n",
    "# print(DQr.policy.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.87754319e-01 1.45798469e-01 1.46779020e-01 1.39356087e-01]\n",
      " [9.88156193e-02 5.06736616e-02 5.82002416e-03 1.32689344e-04]\n",
      " [1.47338169e-01 7.04748094e-03 1.81604922e-02 1.49877929e-02]\n",
      " [8.69438169e-03 0.00000000e+00 0.00000000e+00 9.17740289e-04]\n",
      " [2.19304199e-01 1.29715881e-01 1.31506027e-01 1.42139963e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.68380351e-01 0.00000000e+00 7.38355849e-02 1.40788651e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.70795323e-01 1.63489997e-01 1.50266774e-01 2.91159532e-01]\n",
      " [2.37426909e-01 3.89945047e-01 1.19987124e-01 1.88387631e-01]\n",
      " [4.76912793e-01 1.27023120e-01 1.77637967e-01 5.73585075e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.60096698e-01 3.18045364e-01 5.70826552e-01 2.74698721e-01]\n",
      " [4.23530106e-01 8.11434175e-01 5.80452691e-01 2.97552916e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "test_env.reset()\n",
    "Qr = QL(test_env, n_iter = 5000, gamma = 0.95, alfa = .1, eps = 0.5, d_eps = 0.001)\n",
    "print(Qr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roulette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(1)\n",
      "Discrete(38)\n"
     ]
    }
   ],
   "source": [
    "test_env_2 = gym.make('Roulette-v0')\n",
    "\n",
    "print(test_env_2.observation_space)\n",
    "print(test_env_2.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35.83734247 36.0819095  36.04641939 36.10423133 36.14041359 36.41371308\n",
      "  35.55204465 36.36031831 35.29431239 35.88718556 35.58217124 35.18170958\n",
      "  36.11397648 36.28251724 35.28996592 35.71305149 36.0469198  35.81580764\n",
      "  36.04107616 37.21724014 35.75472604 36.15179887 35.69435257 35.21636144\n",
      "  35.56206542 35.43074812 36.060634   36.20612077 35.30084144 35.81825881\n",
      "  36.04297716 36.38604943 35.61958097 36.38657552 35.54482869 35.70735098\n",
      "  36.62546875 36.17791533]]\n"
     ]
    }
   ],
   "source": [
    "test_env_2.reset()\n",
    "\n",
    "Qri = QL(test_env_2, n_iter=1000, gamma=1, alfa=.1, eps=0.5, d_eps=0.000001)\n",
    "print(Qri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 38)                76        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                624       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 38)                646       \n",
      "=================================================================\n",
      "Total params: 1,618\n",
      "Trainable params: 1,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 100\n",
      "Episode 2: reward: -26.000, steps: 100\n",
      "Episode 3: reward: 85.000, steps: 100\n",
      "Episode 4: reward: -63.000, steps: 100\n",
      "Episode 5: reward: 11.000, steps: 100\n",
      "Episode 6: reward: -26.000, steps: 100\n",
      "Episode 7: reward: 48.000, steps: 100\n",
      "Episode 8: reward: 48.000, steps: 100\n",
      "Episode 9: reward: 11.000, steps: 100\n",
      "Episode 10: reward: -63.000, steps: 100\n",
      "Episode 11: reward: 48.000, steps: 100\n",
      "Episode 12: reward: 48.000, steps: 100\n",
      "Episode 13: reward: 48.000, steps: 100\n",
      "Episode 14: reward: -63.000, steps: 100\n",
      "Episode 15: reward: 48.000, steps: 100\n",
      "Episode 16: reward: 48.000, steps: 100\n",
      "Episode 17: reward: 48.000, steps: 100\n",
      "Episode 18: reward: -26.000, steps: 100\n",
      "Episode 19: reward: -63.000, steps: 100\n",
      "Episode 20: reward: -26.000, steps: 100\n",
      "Episode 21: reward: 11.000, steps: 100\n",
      "Episode 22: reward: -26.000, steps: 100\n",
      "Episode 23: reward: 11.000, steps: 100\n",
      "Episode 24: reward: -63.000, steps: 100\n",
      "Episode 25: reward: 11.000, steps: 100\n",
      "Episode 26: reward: -26.000, steps: 100\n",
      "Episode 27: reward: 11.000, steps: 100\n",
      "Episode 28: reward: -63.000, steps: 100\n",
      "Episode 29: reward: 11.000, steps: 100\n",
      "Episode 30: reward: -100.000, steps: 100\n",
      "Episode 31: reward: 85.000, steps: 100\n",
      "Episode 32: reward: 48.000, steps: 100\n",
      "Episode 33: reward: -63.000, steps: 100\n",
      "Episode 34: reward: -26.000, steps: 100\n",
      "Episode 35: reward: -63.000, steps: 100\n",
      "Episode 36: reward: -26.000, steps: 100\n",
      "Episode 37: reward: -63.000, steps: 100\n",
      "Episode 38: reward: 11.000, steps: 100\n",
      "Episode 39: reward: 122.000, steps: 100\n",
      "Episode 40: reward: 11.000, steps: 100\n",
      "Episode 41: reward: -63.000, steps: 100\n",
      "Episode 42: reward: 48.000, steps: 100\n",
      "Episode 43: reward: 48.000, steps: 100\n",
      "Episode 44: reward: -26.000, steps: 100\n",
      "Episode 45: reward: 122.000, steps: 100\n",
      "Episode 46: reward: -26.000, steps: 100\n",
      "Episode 47: reward: 122.000, steps: 100\n",
      "Episode 48: reward: -63.000, steps: 100\n",
      "Episode 49: reward: 48.000, steps: 100\n",
      "Episode 50: reward: 48.000, steps: 100\n"
     ]
    }
   ],
   "source": [
    "test_env_2.reset()\n",
    "DQr = DeepQL(test_env_2, mem_limit = 1000, batch = 10, warmup = 10, steps = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
