{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning y Deep Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Pablo Melendez\n",
    "- ### Hector Magaña"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje por Reforzamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya sabemos, el objetivo de un agente es actuar o resolver algun problema mediante la ejecucion de alguna accion de acuerdo al ambiente en el que esta y que el mismo agente puede detectar u \"observar\". Tomando en cuenta esto, el aprendizaje por reforzamiento le permite al agente \"aprender\" que accion le conviene realizar de acuerdo al ambiente en el que este se encuentre.\n",
    "\n",
    "Para poder entender mas a fondo este metodo es importante definir lo siguiente:\n",
    "\n",
    "\n",
    "- **Estado:**\n",
    "\n",
    "    El ambiente en el que se encuentra el agente y que al cambiar, puede no ser completamente influenciado por el agente, es decir, que a pesar de la accion que el agente realice el estado puede o no cambiar, y si cambia puede no ser de la manera prevista, es decir es no deterministico.\n",
    "\n",
    "\n",
    "- **Accion:**\n",
    "\n",
    "    Una accion es lo que agente puede realizar en su ambiente por ejemplo, si estamos entrenando un agente para jugar Blackjack, las acciones podrian ser: *Partir, Pedir una carta mas, Quedarse con las cartas que tiene y Retirarse*.\n",
    "\n",
    "\n",
    "- **Recompensa:**\n",
    "\n",
    "    Cada vez que el agente realiza una accion necesita saber si dicha accion fue buena o mala, por lo cual, la recompensa la podemos ver como este valor, donde al realizar una accion negativa podemos obtener una recompensa negativa o nula y si la accion es positiva podemos obtener una recompensa positiva.\n",
    "\n",
    "\n",
    "- **Policy (Poliza):**\n",
    "\n",
    "    Es el conjunto de reglas que va a seguir el agente, en la cual vamos a tener todos los estados del ambiente y la accion que el agente va a realizar cuando se encuentre en cada estado.\n",
    "\n",
    "\n",
    "El objetivo del aprendizaje por reforzamiento es encontrar la poliza donde para cada estado se tenga la accion que le genere la mayor recompensa posible al agente.\n",
    "\n",
    "La base de este metodo de aprendizaje se tiene en el ***Proceso de Decisiones de Markov*** que nos va a permitir generar un modelo donde podemos expresar la relacion del agente con el estado, sus acciones y sus recompensas y la ***Ecuacion de Bellman*** que nos ayuda a encontrar la poliza mas adecuada.\n",
    "\n",
    "- **Proceso de Decisiones de Markov (*MDP, Markov Decision Process*)**:\n",
    "\n",
    "    Nos permite generar un modelo donde se haga visible la relacion que hay entre el agente, las acciones que este puede tomar, las recompensas de dichas acciones y el estado al que puede llevar dicha accion. Por ejemplo:\n",
    "\n",
    "![Proceso de Decisiones de Markov](https://miro.medium.com/max/1280/1*Uh11rrUKKsHLLRmmv0ss2w.jpeg)\n",
    "    En la imagen podemos observar tres diferentes estados para el automovil: *Frio, Caliente y Sobrecalentado*, de igual manera podemos observar las acciones que el automovil puede realizar: *Avanzar Rapido y Avanzar Lento*, los estados a los que estas acciones nos llevan y la probabilidad de que eso ocurra.\n",
    "    \n",
    "    \n",
    "- **Ecuacion de Bellman**\n",
    "\n",
    "\n",
    "$$\n",
    "Q^{*}(s,a)=\\mathbb{E}_{s'\\sim\\epsilon}[r+\\gamma max_{a'}Q^{*}(s',a')|s,a]\n",
    "$$\n",
    "\n",
    "Esta ecuacion nos ayuda a saleccionar la mejor accion *a* para un estado *s*, al escoger la accion que pueda dar una mayor recompensa a futuro.\n",
    "\n",
    "Para esto hay que tomar en cuenta que si una accion en el estado actual nos genera una recompensa, esta recompensa puede disminuir o no ser tan buena a futuro, ahi se explica el valor de $\\gamma$ dentro de la ecuacion, ya que esta indica que la recompensa de ejecutar en el estado *s* la accion *a* va a ser igual a la recompensa que dicha accion *a* genere ($r$) mas la recompensa con descuento de ejecutar la mejor accion *a'* desde el estado *s'* ($\\gamma max_{a'}Q^{*}(s',a')$) donde $\\gamma$ va a ser un factor que reduzca la recompensa que nos de la la mejor accion *a'* para el estado *s'*.\n",
    "\n",
    "#### Algoritmo Q-Learning\n",
    "\n",
    "En el aprendizaje por reforzamiento nos vamos a encontrar con dos \"tipos\": ***Online*** y ***Offline***, cuya diferencia radica en que para el tipo online, el agente conoce la recompensa que le va a generar alguna accion y esta recompensa es negativa, el agente no explora o no va por dicho camino en busca de una poliza, mientras que en el offline el agente tiene que explorar dicha accion para aprender que es mala o que la recompensa que le genera no es buena mientras busca la solucion.\n",
    "\n",
    "<!-- Empieza la descripcion de que es el algoritmo de Q-Learning -->\n",
    "\n",
    "\n",
    "El algoritmo de Q-Learning toma el MDP y la ecuacion de Bellman para realizar de manera iterativa la busqueda de la mejor poliza para resolver el problema planteado, de tal manera que para la primer iteracion de cada estado *s* podemos plantear lo siguiente: ¿Cual es la recompensa de aplicar la acción *a* si parto desde este estado *s*?, para resolver dicha pregunta tenemos que ir al estado *s'* derivado de aplicar la accion *a* en el estado *s* y repetir dicho planteamiento pero ahora partiendo del estado *s'* y aplicando una accion *a'*.\n",
    "\n",
    "Junto con esta modo iterativo de atacar el problema debemos agregarle la \"memoria\" al agente y debemos darle un peso a dicha memoria ya que de no hacerlo podriamos ciclarnos en una serie de estados que no nos lleven a un estado terminal del problema para esto utilizamos varios parametros agregados a la ecuacion de Bellman:\n",
    "\n",
    "- ***Gamma ($\\gamma$)***: Descuento de la recompensa, este valor nos permite alterar el valor de la recompensa obtenida ya que, como ya se menciono antes, cierta accion en cierto estado puede ser bastante buena al inicio de la solucion pero no tanto al final de la misma, teniendo en cuenta que de llegar a un estado que ya hemos procesado antes seria ir en circulos, por lo que este parametro igual ayuda a salir de dicho ciclo.\n",
    "\n",
    "- ***Epsilon***: Le da \"libertad\" al agente, permite que en la busqueda el agente decida realizar o explorar una accion \"no tan buena\" o con menor recompensa aproximada para abarcar una mayor area en el espacio de busqueda. Hay que recordar que estamos buscando un punto maximo en las recompensas por lo que podriamos llegar a tener optimos locales donde el agente puede atorarse o tomar como la mejor solucion, por lo que el permitirle salir de ahi y buscar en otro lado ayuda a encontrar otra opcion que puede ser mejor.\n",
    "\n",
    "- ***Delta Epsilon***: Va restringiendo la libertad que tiene el agente de explorar otras soluciones haciendo que lo aprendido tenga mas peso que una accion aleatoria, esto es debido a que aunque es bueno explorar diferentes acciones al inicio, para poder llegar a una solucion debemos continuar y tener en cuenta lo aprendido para poder obtener un valor de recompensa acertado, en vez de estar tomando acciones al azar todo el tiempo y no llegar a resolver el problema.\n",
    "\n",
    "\n",
    "Con todos estos parametros aplicados a la ecuacion de Bellman y corriendo el algoritmo iterativo, podemos generar una matriz con las dimensiones (estados_posibles x cantidad_acciones) donde  el valor en M\\[s,a\\] es la recompensa esperada de aplicar la accion *a* en el estado *s*. De ahi queda obtener la mejor recompensa y asignar la accion que genera dicha recompensa como la accion a realizar en dicho estado para nuestra poliza final.\n",
    "\n",
    "<!-- Termina la descripcion de Q-Learning -->\n",
    "\n",
    "#### Algoritmo Deep Q-Learning\n",
    "\n",
    "\n",
    "<!-- Empieza la descripcion de que es el algoritmo de Deep Q-Learning -->\n",
    "\n",
    "\n",
    "El algoritmo de Deep Q-Learning reemplaza la tabulacion o matriz generada con una aproximacion de la funcion *Q* mediante el uso de una red neuronal que nos permite filtrar o pulir mas la informacion de las entradas en cada una de las capas de la misma red.\n",
    "\n",
    "<!-- donde la ecuacion de Bellman nos lleva a una serie de funciones de perdida que se buscan minimizar y que cambian con cada iteracion del algoritmo. -->\n",
    "\n",
    "Un ejemplo de este algoritmo lo encontramos en el articulo [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n",
    "\n",
    "Donde se pasa de la ecuacion de Bellman a un par de funciones de perdida:\n",
    "\n",
    "$$\n",
    "L_{i}(\\theta_{i})=\\mathbb{E}_{s,a,r}[(E_{s'}[yD_{s,a}]-Q(s,a;\\theta_{i}))^2]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "L_{i}(\\theta_{i})=\\mathbb{E}_{s,a,r,s'}[(y-Q(s,a;\\theta_{i}))^{2}]+E_{s,a,r}[V_{s'}[y]]\n",
    "$$\n",
    "\n",
    "Estas funciones ayudan a actualizar los pesos usados en la red neuronal en el momento del entrenamiento y como lo explican los autores el ajuste que estas generan junto con el uso de una memoria y una red neuronal con dos capas ocultas mas la capa de entrada y la capa de salida permiten tratar la informacion de manera que el proceso de aprendizaje sea rapido y sin generar la matriz de relacion entre los estados posibles y las acciones.\n",
    "\n",
    "Esto es util cuando nos enfrentamos a problemas de grandes dimensiones donde el espacio o cantidad de estados posibles y la cantidad de acciones son bastante grandes generalmente cuando por ejemplo, nuestro estado no es solamente un tablero donde se nos indica que celdas estan ocupadas y que celdas no, sino cuando aparte de tener en cuenta esto debemos considerar factores como tiempo o \"vida restante\" del agente, en general, resulta util cuando la relacion entre el estado y las acciones no se puede ver de manera lineal o  es dificil representarlo de esta manera.\n",
    "\n",
    "<!-- Termina la descripcion de Deep Q-Learning -->\n",
    "\n",
    "#### Diferencias de Deep Q-Learning y Q-Learning\n",
    "\n",
    "La principal diferencia que encontramos entre estos dos algoritmos, mas alla de las funciones que utilizan para poder determinar en Q-Learning la recompensa de una accion y en Deep Q-Learning las funciones para actualizar los pesos en la red neuronal, es el tamaño del espacio (estado, accion) que pueden manejar, ya que si este espacio es bastante grande al momento de utilizar Q-Learning necesitamos una matriz de dimensiones *(estados_posibles x acciones_posibles)* que nos permita almacenar las diferentes recompensas para cada par (estado, accion), mientras que con Deep Q-Learning la red Neuronal la podemos auxiliar con una memoria de tamaño fijo y que al tener varias capas ocultas nos permite procesar mejor la informacion ya que no solo depende de un factor *gamma* sino que el resultado es influenciado por la salida de todos los elementos de la red que poco a poco iran ajustando sus pesos para llegar al resultado esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dfinimos el algoritmo de Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_accion(Q, s, eps=0.1):\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        return np.argmax(Q[s])\n",
    "\n",
    "def QL(env, n_iter=10000, gamma=0.95, alfa=0.01, eps=0.3, d_eps=0.00005):\n",
    "    \n",
    "    n_a = env.action_space.n\n",
    "    n_s = env.observation_space.n\n",
    "\n",
    "    Q = np.zeros((n_s, n_a))\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        done = False        \n",
    "        state = env.reset()\n",
    "        \n",
    "        if eps > 0.01:\n",
    "            eps -= d_eps\n",
    "        \n",
    "        while not done:\n",
    "            action = sel_accion(Q, state, eps)\n",
    "            next_state, rew, done, info = env.step(action)\n",
    "\n",
    "            Q[state,action] = Q[state,action] + alfa*(rew + gamma*np.max(Q[next_state]) - Q[state,action])\n",
    "            state = next_state\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos el algoritmo de Deep Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepQL(env, mem_limit = 1000, batch = 50, warmup = 2000, steps = 50000):\n",
    "\n",
    "    # Semilla para valores aleatorios\n",
    "    np.random.seed(123)\n",
    "    env.seed(123)\n",
    "    \n",
    "    # Obtenemos la cantidad de acciones\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    # Primera capa\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_actions, input_shape=(1,)+env.observation_space.shape))\n",
    "              \n",
    "    # Capas siguientes\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(nb_actions))\n",
    "              \n",
    "    # Estructura de la red\n",
    "    print(model.summary())\n",
    "\n",
    "    # Creamos la poliza inicial y compilamos el agente\n",
    "    memory = SequentialMemory(limit = mem_limit, window_length = 1)\n",
    "              \n",
    "    policy = BoltzmannQPolicy()\n",
    "              \n",
    "    dqn = DQNAgent(model=model, nb_actions = nb_actions, memory = memory, nb_steps_warmup = warmup,\n",
    "                   target_model_update = 1e-2, policy = policy)\n",
    "              \n",
    "    dqn.compile(Adam(lr = 1e-3), metrics = ['mae'])\n",
    "\n",
    "    # Entrenamos al agente\n",
    "    dqn.fit(env, nb_steps = steps, visualize = False, verbose = 0)\n",
    "\n",
    "    # Testeamos el resultado\n",
    "    dqn.test(env, nb_episodes = 50, visualize = False)\n",
    "    \n",
    "    # Regresamos el agente entrenado\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozen Lake\n",
    "\n",
    "El problema nos pone al agente sobre un lago congelado donde tiene un punto de partida y un punto objetivo al cual llegar, sin embargo la probabilidad de que el agente se mueva de acuerdo a lo que le indiquemos es baja (0.33) y las recompensas tambien por lo que el entrenar al egente es una tarea complicada ya que de acuerdo a la documentacion esto se debe a los \"resbalozo\" que es el hielo y a que el agente unicamente recibe una recompensa al llegar al objetivo. [Ver Docs.](http://gym.openai.com/envs/FrozenLake8x8-v0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make('FrozenLake-v0')\n",
    "\n",
    "test_env.reset()\n",
    "\n",
    "print(test_env.observation_space)\n",
    "print(test_env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 4)                 8         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 700\n",
      "Trainable params: 700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 6\n",
      "Episode 2: reward: 0.000, steps: 5\n",
      "Episode 3: reward: 0.000, steps: 6\n",
      "Episode 4: reward: 0.000, steps: 5\n",
      "Episode 5: reward: 0.000, steps: 5\n",
      "Episode 6: reward: 0.000, steps: 3\n",
      "Episode 7: reward: 0.000, steps: 2\n",
      "Episode 8: reward: 0.000, steps: 7\n",
      "Episode 9: reward: 0.000, steps: 6\n",
      "Episode 10: reward: 0.000, steps: 5\n",
      "Episode 11: reward: 0.000, steps: 10\n",
      "Episode 12: reward: 0.000, steps: 4\n",
      "Episode 13: reward: 0.000, steps: 6\n",
      "Episode 14: reward: 0.000, steps: 3\n",
      "Episode 15: reward: 0.000, steps: 5\n",
      "Episode 16: reward: 0.000, steps: 7\n",
      "Episode 17: reward: 0.000, steps: 3\n",
      "Episode 18: reward: 0.000, steps: 2\n",
      "Episode 19: reward: 0.000, steps: 3\n",
      "Episode 20: reward: 0.000, steps: 2\n",
      "Episode 21: reward: 0.000, steps: 5\n",
      "Episode 22: reward: 0.000, steps: 3\n",
      "Episode 23: reward: 0.000, steps: 4\n",
      "Episode 24: reward: 0.000, steps: 9\n",
      "Episode 25: reward: 0.000, steps: 2\n",
      "Episode 26: reward: 0.000, steps: 3\n",
      "Episode 27: reward: 0.000, steps: 7\n",
      "Episode 28: reward: 0.000, steps: 4\n",
      "Episode 29: reward: 0.000, steps: 5\n",
      "Episode 30: reward: 0.000, steps: 4\n",
      "Episode 31: reward: 0.000, steps: 3\n",
      "Episode 32: reward: 0.000, steps: 8\n",
      "Episode 33: reward: 0.000, steps: 6\n",
      "Episode 34: reward: 0.000, steps: 4\n",
      "Episode 35: reward: 0.000, steps: 7\n",
      "Episode 36: reward: 1.000, steps: 16\n",
      "Episode 37: reward: 0.000, steps: 4\n",
      "Episode 38: reward: 0.000, steps: 5\n",
      "Episode 39: reward: 0.000, steps: 3\n",
      "Episode 40: reward: 0.000, steps: 5\n",
      "Episode 41: reward: 0.000, steps: 2\n",
      "Episode 42: reward: 0.000, steps: 12\n",
      "Episode 43: reward: 0.000, steps: 4\n",
      "Episode 44: reward: 0.000, steps: 29\n",
      "Episode 45: reward: 0.000, steps: 3\n",
      "Episode 46: reward: 0.000, steps: 2\n",
      "Episode 47: reward: 0.000, steps: 5\n",
      "Episode 48: reward: 0.000, steps: 6\n",
      "Episode 49: reward: 0.000, steps: 5\n",
      "Episode 50: reward: 0.000, steps: 3\n"
     ]
    }
   ],
   "source": [
    "test_env.reset()\n",
    "DQr = DeepQL(test_env, mem_limit = 1000, batch = 10, warmup = 10000, steps = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18630311e-01 1.18979672e-01 1.31370423e-01 1.18660470e-01]\n",
      " [1.39348559e-02 2.10387239e-02 2.04852622e-02 1.27388384e-01]\n",
      " [1.32858494e-01 3.89747531e-02 6.07853606e-03 1.39450314e-02]\n",
      " [7.30197453e-06 0.00000000e+00 0.00000000e+00 2.10252726e-02]\n",
      " [1.60522967e-01 1.29935327e-01 1.05796188e-01 5.17762098e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.88762065e-02 6.22666909e-02 1.46565200e-01 1.88092752e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.80830987e-02 1.31987004e-01 1.01341796e-01 2.47658546e-01]\n",
      " [8.69250533e-02 3.72808709e-01 1.63852343e-01 6.29972954e-02]\n",
      " [4.84533777e-01 1.64119935e-01 1.33039663e-01 6.50687479e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.34408048e-02 1.92051637e-01 4.45528023e-01 1.08268258e-01]\n",
      " [1.14792926e-01 7.16593992e-01 4.91755211e-01 2.25493105e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "test_env.reset()\n",
    "Qr = QL(test_env, n_iter = 1000, gamma = 0.95, alfa = .1, eps = 0.5, d_eps = 0.001)\n",
    "print(Qr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "obs = test_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = np.argmax(Qr[obs])\n",
    "    obs, r, done, info = test_env.step(a)\n",
    "    test_env.render()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roulette\n",
    "\n",
    "Este ambiente busca enseñar a jugar a la ruleta de los casinos donde el agente apuesta por un numero y recibe una recompensa positiva si el numero que resulta en la ruleta es diferente de cero y la paridad de dicho numero es igual a la paridad del numero por el que aposto el agente y de igual manera el agente tiene la opcion de retirarse del juego y terminar el ambiente.\n",
    "\n",
    "En este caso el parametro gamma es casi 1 debido a que la decision que tome el agente en un tiro de la ruleta, no influye en el resultado de tiros posteriores por lo que el descuento de la recompensa de un siguiente intento es casi nulo.\n",
    "\n",
    "De igual manera alfa que ayuda a determinar el aumento o disminucion de la recompensa para una accion es muy bajo ya que los resultados anteriores.\n",
    "\n",
    "Epsilon lo establecimos a la mitad para que tenga la misma probabilidad de escoger una accion aleatoria o continuar con la accion (apuesta) que ha estado haciendo (es decir, siga con lo aprendido) y finalmente delta epsilon lo pusimos a un numero muy bajo para que esa probabilidad de que el agente escoja algun numero aleatorio o continue con la apuesta que ha estado haciendo se mantenga lo mas igual posible sin volverla cero porque eso significaria no tomar en cuenta o no darle cada vez mas peso a lo aprendido.\n",
    "\n",
    "Otra razon de el valor de epsilon y delta epsilon es que al asignarles valores mas grandes, el agente va a aprender que es mejor tomar el camino de retirarse sin perder que el apostar por algun numero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(1)\n",
      "Discrete(38)\n"
     ]
    }
   ],
   "source": [
    "test_env_2 = gym.make('Roulette-v0')\n",
    "\n",
    "print(test_env_2.observation_space)\n",
    "print(test_env_2.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.78457281e-01 -1.86200459e-01 -2.05500698e-01 -3.77238749e-02\n",
      "  -1.75521658e-01 -3.29581881e-02 -1.49804470e-01 -2.50040380e-01\n",
      "  -1.25758798e-01 -1.19733532e-02 -2.04155468e-01 -2.06866312e-03\n",
      "  -3.78927195e-02 -1.73921741e-01 -5.37172381e-02 -1.69503908e-01\n",
      "  -6.92683833e-02 -2.20369729e-01 -2.23411254e-01 -2.66097531e-02\n",
      "  -1.15099913e-01 -5.58173073e-02 -1.09533165e-01 -1.16855642e-01\n",
      "  -1.94613963e-01 -1.16735719e-01 -3.30731918e-02 -1.63721081e-01\n",
      "  -1.08644711e-02 -3.83112401e-02 -1.71395985e-01 -3.23493715e-01\n",
      "  -4.68043455e-02 -8.95599103e-02 -1.62216612e-01 -1.63071902e-01\n",
      "  -1.50724762e-01  7.92373158e-28]]\n"
     ]
    }
   ],
   "source": [
    "test_env_2.reset()\n",
    "\n",
    "Qri = QL(test_env_2, n_iter=10000, gamma=.9, alfa=.1, eps=0.5, d_eps=0.000001)\n",
    "print(Qri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "guess: 25, reward: -1.0\n",
      "guess: 25, reward: 1.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "obs = test_env_2.reset()\n",
    "done = False\n",
    "final_r = 0\n",
    "a = -10000000000\n",
    "ind =0\n",
    "it =0\n",
    "for i in Qri[0]:\n",
    "    if i > a:\n",
    "        ind = it\n",
    "        a = i\n",
    "    it = it + 1\n",
    "while not done:\n",
    "    obs, r, done, info = test_env_2.step(ind)\n",
    "    final_r = final_r + r\n",
    "    print(\"guess: {}, reward: {}\".format(ind, r))\n",
    "print(final_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 38)                76        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                624       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 38)                646       \n",
      "=================================================================\n",
      "Total params: 1,890\n",
      "Trainable params: 1,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 100\n",
      "Episode 2: reward: -26.000, steps: 100\n",
      "Episode 3: reward: 85.000, steps: 100\n",
      "Episode 4: reward: -63.000, steps: 100\n",
      "Episode 5: reward: 11.000, steps: 100\n",
      "Episode 6: reward: -26.000, steps: 100\n",
      "Episode 7: reward: 48.000, steps: 100\n",
      "Episode 8: reward: 48.000, steps: 100\n",
      "Episode 9: reward: 11.000, steps: 100\n",
      "Episode 10: reward: -63.000, steps: 100\n",
      "Episode 11: reward: 48.000, steps: 100\n",
      "Episode 12: reward: 48.000, steps: 100\n",
      "Episode 13: reward: 48.000, steps: 100\n",
      "Episode 14: reward: -63.000, steps: 100\n",
      "Episode 15: reward: 48.000, steps: 100\n",
      "Episode 16: reward: 48.000, steps: 100\n",
      "Episode 17: reward: 48.000, steps: 100\n",
      "Episode 18: reward: -26.000, steps: 100\n",
      "Episode 19: reward: -63.000, steps: 100\n",
      "Episode 20: reward: -26.000, steps: 100\n",
      "Episode 21: reward: 11.000, steps: 100\n",
      "Episode 22: reward: -26.000, steps: 100\n",
      "Episode 23: reward: 11.000, steps: 100\n",
      "Episode 24: reward: -63.000, steps: 100\n",
      "Episode 25: reward: 11.000, steps: 100\n",
      "Episode 26: reward: -26.000, steps: 100\n",
      "Episode 27: reward: 11.000, steps: 100\n",
      "Episode 28: reward: -63.000, steps: 100\n",
      "Episode 29: reward: 11.000, steps: 100\n",
      "Episode 30: reward: -100.000, steps: 100\n",
      "Episode 31: reward: 85.000, steps: 100\n",
      "Episode 32: reward: 48.000, steps: 100\n",
      "Episode 33: reward: -63.000, steps: 100\n",
      "Episode 34: reward: -26.000, steps: 100\n",
      "Episode 35: reward: -63.000, steps: 100\n",
      "Episode 36: reward: -26.000, steps: 100\n",
      "Episode 37: reward: -63.000, steps: 100\n",
      "Episode 38: reward: 11.000, steps: 100\n",
      "Episode 39: reward: 122.000, steps: 100\n",
      "Episode 40: reward: 11.000, steps: 100\n",
      "Episode 41: reward: -63.000, steps: 100\n",
      "Episode 42: reward: 48.000, steps: 100\n",
      "Episode 43: reward: 48.000, steps: 100\n",
      "Episode 44: reward: -26.000, steps: 100\n",
      "Episode 45: reward: 122.000, steps: 100\n",
      "Episode 46: reward: -26.000, steps: 100\n",
      "Episode 47: reward: 122.000, steps: 100\n",
      "Episode 48: reward: -63.000, steps: 100\n",
      "Episode 49: reward: 48.000, steps: 100\n",
      "Episode 50: reward: 48.000, steps: 100\n"
     ]
    }
   ],
   "source": [
    "test_env_2.reset()\n",
    "DQr = DeepQL(test_env_2, mem_limit = 1000, batch = 10, warmup = 10, steps = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
